{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START EDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_events_file = 'events\\ASUx-ENG101x-3T2015_events.csv'\n",
    "course_structure_file = 'course_structure_files\\ASUx-ENG101x-2171A.csv'\n",
    "username_id = 'username_id.csv'\n",
    "\n",
    "events_course_id = 'course-v1:ASUx+ENG101x+2171A' \n",
    "course_structure_course_id = 'ASUx-ENG101x-2171A'\n",
    "\n",
    "\n",
    "output_file = 'combined_events/course_structure_events_join_ASUx+ENG101x+2171A.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END EDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import datetime \n",
    "import warnings \n",
    "import json\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('max_colwidth',1000)\n",
    "pd.set_option('display.max_columns',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in events file \n",
    "events = pd.read_csv(input_events_file)\n",
    "\n",
    "# read in course structure file \n",
    "course_structure = pd.read_csv(course_structure_file)\n",
    "\n",
    "# read in username_id file -- this will translate the usernames to ids \n",
    "username_id = pd.read_csv(username_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert course_structure columns to all lowercase \n",
    "course_structure.columns = map(str.lower, course_structure.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse out the user_ids from the events column, if available \n",
    "\n",
    "def parse_user_id(x):\n",
    "    try:\n",
    "        string = json.loads(x)\n",
    "        if \"user_id\" in string:\n",
    "            return string[\"user_id\"]\n",
    "    except ValueError:\n",
    "        return np.NaN\n",
    "\n",
    "events[\"user_id\"] = events.apply(lambda x: parse_user_id(x['event']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "username_id['id'] = username_id['id'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# join to username_id table \n",
    "\n",
    "events = events.merge(username_id, how = 'left', left_on = 'user_id', right_on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_username(x,y):\n",
    "    if pd.isnull(x) and pd.notnull(y):\n",
    "        return y\n",
    "    else:\n",
    "        return x \n",
    "\n",
    "events['username'] = events.apply(lambda x: change_username(x['username_x'],x['username_y']),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = events.drop(['username_x','username_y','user_id','id'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time column to datetime \n",
    "events['time'] = pd.to_datetime(events['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove record where event is NA \n",
    "events = events.dropna(subset = ['name'])\n",
    "\n",
    "# remove records where event = page_close \n",
    "events = events[events['name'] != 'page_close']\n",
    "\n",
    "# remove irrelevant courses --  CSE110x fall course \n",
    "events = events[events['context_course_id'] == events_course_id]\n",
    "course_structure = course_structure[course_structure['folder_course_id']==course_structure_course_id]\n",
    "\n",
    "# reset index \n",
    "events = events.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event_name = edx.ui.lms.link_clicked, remove current_url so that the only one left is the the target_url. \n",
    "# the current_url is where the user is navigating FROM, which we don't care about. \n",
    "\n",
    "def remove_current_url(event, event_name):\n",
    "    if event_name == 'edx.ui.lms.link_clicked':\n",
    "        try:\n",
    "            event_json = json.loads(event)\n",
    "            return event_json['target_url']\n",
    "        except ValueError:\n",
    "            return \"\"\n",
    "    else:\n",
    "        return event\n",
    "\n",
    "events['old_event'] = events['event']\n",
    "events['event'] = events.apply(lambda x: remove_current_url(x['old_event'],x['name']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only the necessary columns from course structure table \n",
    "course_structure_index = course_structure.loc[:,[    \n",
    "    'chapter_id', 'chapter_order', 'chapter_display_name', 'chapter_start',\n",
    "    'vertical_id', 'vertical_order', 'vertical_display_name',\n",
    "    'sequential_id', 'sequential_order', 'sequential_display_name',\n",
    "    'smallestunit_discussion_id', 'smallestunit_id', 'smallestunit_order',\n",
    "    'smallestunit_type', 'smallestunit_display_name', \n",
    "    'course_end','course_enrollment_end','course_start'\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## obtain_id Function \n",
    "\n",
    "Purpose: Separate all of the ids (chapter_id, sequential_id, vertical_id, smallestunit_discussion_id, smallestunit_id) in the 'event' column of the tracking logs so that we can match it to the ids course_structure file. This function will look for the 5 different id types and create new columns to store them in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ids = ['chapter_id','vertical_id','sequential_id','smallestunit_discussion_id','smallestunit_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_id(id_type,output):\n",
    "    # create series of event names in the events (log) table \n",
    "    event_names = events['event'] \n",
    "    \n",
    "    # get a list of all of the unique ids from the course structure table\n",
    "    unique = course_structure[id_type].unique()\n",
    "\n",
    "    # for each unique id, see if it's in any part of the events \n",
    "    lst = []\n",
    "    for i in unique:\n",
    "        sub_lst = [] \n",
    "        contains_string = pd.DataFrame(event_names.str.contains(i, regex = False))\n",
    "        sub_lst.append(contains_string[contains_string['event'] == True].index.values)\n",
    "        sub_lst.append(i)\n",
    "        lst.append(sub_lst)\n",
    "\n",
    "    lst_df = pd.DataFrame(lst)\n",
    "    lst_df['count'] = lst_df[0].apply(lambda x: len(x))\n",
    "    lst_df = lst_df[lst_df['count'] != 0]\n",
    "\n",
    "    df = pd.DataFrame(columns = [id_type,'index']) \n",
    "    for i, row in lst_df.iterrows(): \n",
    "        for i in row[0]:\n",
    "            df = df.append({'index':i, id_type:row[1]}, ignore_index = True)\n",
    "\n",
    "    events['index'] = events.index\n",
    "    events['index'] = events['index'].astype('int64')\n",
    "    df['index'] = df['index'].astype('int64')\n",
    "    output = df.merge(events.loc[:,['index','name']], on = 'index')\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "# initialize outputs to dataframe\n",
    "output1 = pd.DataFrame\n",
    "output2 = pd.DataFrame\n",
    "output3 = pd.DataFrame\n",
    "output4 = pd.DataFrame\n",
    "output5 = pd.DataFrame\n",
    "\n",
    "\n",
    "# Feed into obtain-id function \n",
    "\n",
    "chapter_id_output = obtain_id('chapter_id',output1)\n",
    "vertical_id_output = obtain_id('vertical_id',output2)\n",
    "sequential_id_output = obtain_id('sequential_id',output3)\n",
    "smallestunitdiscussion_id_output = obtain_id('smallestunit_discussion_id',output4)\n",
    "smallestunit_id_output = obtain_id('smallestunit_id',output5)\n",
    "\n",
    "\n",
    "# join outputs into one table \n",
    "\n",
    "final_output = chapter_id_output.merge(\n",
    "    vertical_id_output, how = 'outer',on = 'index').merge(\n",
    "    sequential_id_output, how = 'outer',on = 'index').merge(\n",
    "    smallestunitdiscussion_id_output, how = 'outer',on = 'index').merge(\n",
    "    smallestunit_id_output, how = 'outer',on = 'index')\n",
    "\n",
    "\n",
    "# consolidate all names and drop rest of name columns \n",
    "final_output['final_name'] = final_output.loc[:,['name','name_x','name_y']].bfill(axis=1).iloc[:,0]\n",
    "\n",
    "final_output = final_output.drop(['name','name_x','name_y'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge final_output with event log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testing_merge = events.merge(final_output, on = 'index', how = 'inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Course Structure Re-Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# replace null smallestunit_discussion_ids with smallest_unit_id\n",
    "# this is because the we have to standardize the hierarchies. also did the same thing with the course_structure data \n",
    "testing_merge['smallestunit_discussion_id'].fillna(testing_merge['smallestunit_id'], inplace = True)\n",
    "\n",
    "# drop column \n",
    "testing_merge = testing_merge.drop(['smallestunit_id'], axis = 1)\n",
    "\n",
    "# rename column \n",
    "testing_merge = testing_merge.rename(columns={\"smallestunit_discussion_id\":\"smallestunit_id\"})\n",
    "\n",
    "#### NEED TO TEST THIS LATER TO MAKE SURE IT'S CORRECT ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if the smallest unit is a discussion, make the name of the new column the smallestunit_discussion_id. Otherwise, keep it as smallestunit_id\n",
    "\n",
    "def smallestunit_name(smallestunit_type,smallestunit_id,smallestunit_discussion_id):\n",
    "    if smallestunit_type == 'discussion':\n",
    "        value = smallestunit_discussion_id\n",
    "    else:\n",
    "        value = smallestunit_id\n",
    "    return value\n",
    "\n",
    "course_structure_index['smallestunit_incl_dis'] = course_structure_index.apply(\n",
    "    lambda x: smallestunit_name(x['smallestunit_type']\n",
    "                                ,x['smallestunit_id']\n",
    "                                ,x['smallestunit_discussion_id']), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# drop unused columns\n",
    "course_structure_index = course_structure_index.drop(['smallestunit_id','smallestunit_discussion_id','smallestunit_type'],axis=1)\n",
    "\n",
    "# rename column \n",
    "course_structure_index = course_structure_index.rename(columns={\"smallestunit_incl_dis\":\"smallestunit_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column that has the id type \n",
    "def find_id_level(sequential_id,vertical_id,smallestunit_id):\n",
    "    if pd.notnull(smallestunit_id):\n",
    "        value = \"smallestunit_id\"\n",
    "    elif pd.notnull(vertical_id):\n",
    "        value = \"vertical_id\"\n",
    "    elif pd.notnull(sequential_id):\n",
    "        value = \"sequential_id\"\n",
    "    else:\n",
    "        value = \"chapter_id\"\n",
    "    return value \n",
    "\n",
    "testing_merge['id_level'] = testing_merge.apply(lambda x: find_id_level(\n",
    "    x['sequential_id'],x['vertical_id'],x['smallestunit_id']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# groupby IDs\n",
    "chapter_id_gb = course_structure_index.groupby('chapter_id', as_index = False).first()[\n",
    "    ['chapter_id','chapter_order','chapter_display_name', 'chapter_start',\n",
    "     'course_end','course_enrollment_end','course_start']]\n",
    "\n",
    "sequential_id_gb = course_structure_index.groupby(\n",
    "    ['chapter_id','sequential_id'], as_index = False).first()[\n",
    "    ['chapter_id','chapter_order','chapter_display_name', 'chapter_start',\n",
    "     'sequential_id','sequential_order','sequential_display_name',\n",
    "     'course_end','course_enrollment_end','course_start']]\n",
    "\n",
    "vertical_id_gb = course_structure_index.groupby(\n",
    "    ['chapter_id','sequential_id','vertical_id'], as_index = False).first()[\n",
    "    ['chapter_id','chapter_order','chapter_display_name', 'chapter_start',\n",
    "     'sequential_id','sequential_order','sequential_display_name',\n",
    "     'vertical_id','vertical_order','vertical_display_name',\n",
    "     'course_end','course_enrollment_end','course_start']]\n",
    "\n",
    "\n",
    "smallestunit_id_gb = course_structure_index.groupby(\n",
    "    ['chapter_id','sequential_id','vertical_id','smallestunit_id'], as_index = False).first()[\n",
    "    ['chapter_id','chapter_order', 'chapter_display_name', 'chapter_start',\n",
    "     'sequential_id','sequential_order','sequential_display_name',\n",
    "     'vertical_id','vertical_order','vertical_display_name',\n",
    "     'smallestunit_id','smallestunit_order','smallestunit_display_name',\n",
    "     'course_end','course_enrollment_end','course_start']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate testing_merge file by the id_level column \n",
    "chapter_id_events = testing_merge[testing_merge['id_level'] == 'chapter_id']\n",
    "sequential_id_events = testing_merge[testing_merge['id_level'] == 'sequential_id']\n",
    "vertical_id_events = testing_merge[testing_merge['id_level'] == 'vertical_id']\n",
    "smallestunit_id_events = testing_merge[testing_merge['id_level'] == 'smallestunit_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge each file based on the id_levels \n",
    "\n",
    "concat1 = chapter_id_gb.merge(chapter_id_events, how = 'inner', on = 'chapter_id')\n",
    "concat2 = sequential_id_gb.merge(sequential_id_events.drop(\n",
    "    ['chapter_id'],axis=1), how = 'inner', on = 'sequential_id')\n",
    "concat3 = vertical_id_gb.merge(vertical_id_events.drop(\n",
    "    ['chapter_id','sequential_id'],axis=1), how = 'inner', on = 'vertical_id')\n",
    "concat4 = smallestunit_id_gb.merge(smallestunit_id_events.drop(\n",
    "    ['chapter_id','sequential_id','vertical_id'],axis=1), how = 'inner', on = 'smallestunit_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_all = pd.concat([concat1, concat2, concat3, concat4])\n",
    "concat_all.to_csv(output_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_all.to_csv(output_file, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
